{"fullname": "Bo Li", "title": "Assistant Professor", "contact": {"phone": "(217) 300-8141", "email": "lbo@illinois.edu"}, "office": "4310 Siebel Center for Comp Sci", "education": [{"degree": "", "field": "", "institution": "", "year": 0}], "biography": "Bo Li is an Assistant Professor at the University of Illinois at Urbana-Champaign. Her research focuses on devising new ways to fool AI, which she uses to make AI systems more robust. She has shown that adversarial attacks, which manipulate input data to fool neural networks, can be performed in the physical world, not just digitally. Her work includes creating subtle changes in the features of physical objects that are imperceptible to humans but can make the objects invisible to image recognition algorithms. She uses one neural network to identify and exploit vulnerabilities in another, developing strategies to patch these flaws and defend against future attacks. Her techniques are being used in commercial applications by companies like IBM and Amazon to protect their AI systems, and by autonomous vehicle companies to improve the robustness of their machine-learning models.", "professionalHighlights": [{"position": "Assistant Professor", "organization": "Siebel School of Computing and Data Science, Illinois", "yearStart": 0, "yearEnd": null}], "researchStatement": "My research focuses on trustworthy machine learning, with an emphasis on robustness, privacy, generalization, and their interconnections. We believe that closing today's trustworthiness gap in ML requires us to tackle these grappled problems in a holistic framework, driven by fundamental research focusing on not only each problem but also their underlying interactions. The long-term goal for our group, Secure learning lab (SL2), is to make machine learning systems robust, private, and generalizable with guarantees for different real-world applications. We have worked on exploring different types of adversarial attacks, including evasion and poisoning attacks in digital and physical worlds, under various constraints. We have developed and will continue to explore robust learning systems based on game-theoretic analysis, knowledge-enabled logical reasoning, and properties of learning tasks. Our work directly benefits applications such as computer vision, natural language processing, safe autonomous driving, and trustworthy federated learning systems.", "researchInterests": [{"area": "Information Theory", "description": ""}, {"area": "Game Theory", "description": ""}, {"area": "Privacy", "description": ""}, {"area": "Trustworthy Machine Learning", "description": ""}, {"area": "Security", "description": ""}, {"area": "Artificial Intelligence", "description": ""}], "researchAreas": ["Artificial Intelligence", "Security and Privacy"], "publications": [{"title": "Robust physical-world attacks on deep learning visual classification", "authors": ["K Eykholt", "I Evtimov", "E Fernandes", "B Li", "A Rahmati", "C Xiao", "A Prakash", "T Wang", "X Wang"], "conference": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "year": 2018}, {"title": "Targeted backdoor attacks on deep learning systems using data poisoning", "authors": ["X Chen", "C Liu", "B Li", "K Lu", "D Song"], "conference": "arXiv preprint arXiv:1712.05526", "year": 2017}, {"title": "Generating adversarial examples with adversarial networks", "authors": ["C Xiao", "B Li", "JY Zhu", "W He", "M Liu", "D Song"], "conference": "arXiv preprint arXiv:1801.02610", "year": 2018}, {"title": "Manipulating machine learning: Poisoning attacks and countermeasures for regression learning", "authors": ["M Jagielski", "A Oprea", "B Biggio", "C Liu", "C Nita-Rotaru", "B Li"], "conference": "2018 IEEE Symposium on Security and Privacy (SP)", "year": 2018}, {"title": "Characterizing adversarial subspaces using local intrinsic dimensionality", "authors": ["X Ma", "B Li", "Y Wang", "SM Erfani", "S Wijewickrema", "G Schoenebeck", "D Song", "C Chen", "L Li", "Y Liu"], "conference": "arXiv preprint arXiv:1801.02613", "year": 2018}, {"title": "Textbugger: Generating adversarial text against real-world applications", "authors": ["J Li", "S Ji", "T Du", "B Li", "T Wang"], "conference": "arXiv preprint arXiv:1812.05271", "year": 2018}, {"title": "Deepgauge: Multi-granularity testing criteria for deep learning systems", "authors": ["L Ma", "F Juefei-Xu", "F Zhang", "J Sun", "M Xue", "B Li", "C Chen", "T Su", "L Li", "Y Liu"], "conference": "Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering", "year": 2018}, {"title": "DBA: Distributed Backdoor Attacks against Federated Learning", "authors": ["C Xie", "K Huang", "PY Chen", "B Li"], "conference": "International Conference on Learning Representations", "year": 2019}, {"title": "Spatially transformed adversarial examples", "authors": ["C Xiao", "JY Zhu", "B Li", "W He", "M Liu", "D Song"], "conference": "arXiv preprint arXiv:1801.02612", "year": 2018}, {"title": "Physical adversarial examples for object detectors", "authors": ["D Song", "K Eykholt", "I Evtimov", "E Fernandes", "B Li", "A Rahmati", "F Tramer", "T Wang"], "conference": "12th USENIX Workshop on Offensive Technologies (WOOT 18)", "year": 2018}, {"title": "The secret revealer: generative model-inversion attacks against deep neural networks", "authors": ["Y Zhang", "R Jia", "H Pei", "W Wang", "B Li", "D Song"], "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2020}, {"title": "Towards efficient data valuation based on the shapley value", "authors": ["R Jia", "D Dao", "B Wang", "FA Hubis", "N Hynes", "NM G\u00fcrel", "B Li", "C Zhang", "D Song"], "conference": "The 22nd International Conference on Artificial Intelligence and Statistics", "year": 2019}, {"title": "Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks", "authors": ["Y Li", "X Lyu", "N Koren", "L Lyu", "B Li", "X Ma"], "conference": "arXiv preprint arXiv:2101.05930", "year": 2021}, {"title": "Deephunter: A coverage-guided fuzz testing framework for deep neural networks", "authors": ["X Xie", "L Ma", "F Juefei-Xu", "M Xue", "H Chen", "Y Liu", "J Zhao", "B Li", "J Yin", "S See"], "conference": "Proceedings of the 28th ACM SIGSOFT International Symposium on Software Engineering", "year": 2019}, {"title": "Deepmutation: Mutation testing of deep learning systems", "authors": ["L Ma", "F Zhang", "J Sun", "M Xue", "B Li", "F Juefei-Xu", "C Xie", "L Li", "Y Liu", "J Zhao"], "conference": "2018 IEEE 29th International Symposium on Software Reliability Engineering", "year": 2018}, {"title": "Data poisoning attacks on factorization-based collaborative filtering", "authors": ["B Li", "Y Wang", "A Singh", "Y Vorobeychik"], "conference": "Advances in Neural Information Processing Systems", "year": 2016}, {"title": "Data Poisoning Attacks on Factorization-based Collaborative Filtering", "authors": ["B Li", "Y Wang", "A Singh", "Y Vorobeychik"], "conference": "Proceedings of the Neural Information Processing Systems (NIPS)", "year": 2016}, {"title": "Towards stable and efficient training of verifiably robust neural networks", "authors": ["H Zhang", "H Chen", "C Xiao", "S Gowal", "R Stanforth", "B Li", "D Boning", "CJ Hsieh"], "conference": "arXiv preprint arXiv:1906.06316", "year": 2019}, {"title": "Adversarial attack and defense on graph data: A survey", "authors": ["L Sun", "Y Dou", "C Yang", "K Zhang", "J Wang", "SY Philip", "L He", "B Li"], "conference": "IEEE Transactions on Knowledge and Data Engineering", "year": 2022}], "teachingHonors": [{"honor": "Teachers Ranked as Excellent Award", "year": 2021}], "researchHonors": [{"honor": "IJCAI Computers and Thought Award", "organization": "International Joint Conference on Artificial Intelligence", "year": 2022}, {"honor": "AI's 10 to Watch", "organization": "", "year": 2022}, {"honor": "Google Faculty Research Award", "organization": "", "year": 2022}, {"honor": "First prize in the International Verification Neural Networks Competition (VNN-COMP'22)", "organization": "", "year": 2022}, {"honor": "Dean's Award for Excellence in Research", "organization": "", "year": 2022}, {"honor": "C.W. Gear Outstanding Junior Faculty Award", "organization": "", "year": 2021}, {"honor": "Facebook Research Award", "organization": "", "year": 2021}, {"honor": "Best Paper Award in EWSN", "organization": "", "year": 2021}, {"honor": "Intel's 2020 Rising Star Faculty Award", "organization": "", "year": 2020}, {"honor": "Amazon Research Award", "organization": "", "year": 2020}, {"honor": "NSF CAREER Award", "organization": "", "year": 2020}, {"honor": "MIT Technology Review 35 Innovators Under 35", "organization": "", "year": 2020}, {"honor": "Facebook Research Award", "organization": "", "year": 2019}, {"honor": "Amazon Research Award", "organization": "", "year": 2019}, {"honor": "IBM Research Award", "organization": "", "year": 2019}, {"honor": "Q4 AWS Machine Learning Research Awards", "organization": "", "year": 2019}], "coursesTaught": [{"code": "CS 307", "title": "Model & Learning in Data Sci"}, {"code": "CS 442 (CS 498 LB1, CS 498 LB2)", "title": "Trustworthy Machine Learning"}, {"code": "CS 562", "title": "Adv Topics in Sec, Priv and ML"}, {"code": "CS 598 BL", "title": "Adversarial Machine Learning"}]}