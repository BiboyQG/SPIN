{"fullname": "Nan Jiang", "title": "Associate Professor", "contact": {"phone": "(217) 300-8275", "email": "nanjiang@illinois.edu"}, "office": "3322 Siebel Center for Comp Sci", "education": [{"degree": "PhD", "field": "Computer Science and Engineering", "institution": "University of Michigan", "year": 2017}, {"degree": "Bachelor of Engineering", "field": "Automation", "institution": "Tsinghua University", "year": 2011}], "biography": "Nan Jiang is a machine learning researcher working on building the theoretical foundation of reinforcement learning (RL), especially in the function-approximation setting. He is open to collaboration on applying RL to various domains.", "professionalHighlights": [{"position": "Action Editor", "organization": "Journal of Machine Learning Research (JMLR)", "yearStart": 2024, "yearEnd": null}, {"position": "Editor", "organization": "Foundations and Trends in Machine Learning (FnT ML)", "yearStart": 2023, "yearEnd": null}], "researchStatement": "Nan Jiang's research focuses on the theoretical foundations of reinforcement learning, particularly in the function-approximation setting. He is also interested in applying RL to various domains, including robotics, adaptive medical treatment, online recommendation systems, and more. His recent work includes developing novel methods for offline reinforcement learning, which aims to address the challenges of learning from historical data without direct interactions with the real environment. He is currently collaborating with researchers from 10 other universities as part of the new AI Institute for Future Edge Networks and Distributed Intelligence (AI-EDGE) to explore how AI can help optimize computer network operations. The AI-EDGE Institute is one of the $20M AI Institutes funded by the National Science Foundation and the Department of Homeland Security to create a research, education, knowledge transfer, and workforce development environment that will help develop the United State\u2019s technological leadership in future generation edge networks (6G and beyond) and distributed AI for many decades to come.", "researchInterests": [{"area": "Reinforcement Learning", "description": "Building the theoretical foundation of reinforcement learning, especially in the function-approximation setting."}], "researchAreas": ["Artificial Intelligence"], "publications": [{"title": "On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation", "authors": ["Yuheng Zhang", "Nan Jiang"], "conference": "NeurIPS-24", "year": 2024}, {"title": "Future-Dependent Value-Based Off-Policy Evaluation in POMDPs", "authors": ["Masatoshi Uehara", "Haruka Kiyohara", "Andrew Bennett", "Victor Chernozhukov", "Nan Jiang", "Nathan Kallus", "Chengchun Shi", "Wen Sun"], "conference": "NeurIPS-23", "year": 2023}, {"title": "Reinforcement Learning in Low-Rank MDPs with Density Features", "authors": ["Audrey Huang", "Jinglin Chen", "Nan Jiang"], "conference": "ICML-23", "year": 2023}, {"title": "Offline Reinforcement Learning with Realizability and Single-policy Concentrability", "authors": ["Wenhao Zhan", "Baihe Huang", "Audrey Huang", "Nan Jiang", "Jason D. Lee"], "conference": "COLT-22", "year": 2022}, {"title": "Adversarially Trained Actor Critic for Offline Reinforcement Learning", "authors": ["Ching-An Cheng", "Tengyang Xie", "Nan Jiang", "Alekh Agarwal"], "conference": "ICML-22", "year": 2022}, {"title": "Towards Hyperparameter-free Policy Selection for Offline Reinforcement Learning", "authors": ["Siyuan Zhang", "Nan Jiang"], "conference": "NeurIPS-21", "year": 2021}, {"title": "On Query-efficient Planning in MDPs under Linear Realizability of the Optimal State-value Function", "authors": ["Gellert Weisz", "Philip Amortila", "Barnab\u00e1s Janzer", "Yasin Abbasi-Yadkori", "Nan Jiang", "Csaba Szepesv\u00e1ri"], "conference": "COLT-21", "year": 2021}, {"title": "Batch Value-function Approximation with Only Realizability", "authors": ["Tengyang Xie", "Nan Jiang"], "conference": "ICML-21", "year": 2021}, {"title": "Minimax Weight and Q-Function Learning for Off-Policy Evaluation", "authors": ["Masatoshi Uehara", "Jiawei Huang", "Nan Jiang"], "conference": "ICML-20", "year": 2020}, {"title": "Information-Theoretic Considerations in Batch Reinforcement Learning", "authors": ["Jinglin Chen", "Nan Jiang"], "conference": "ICML-19", "year": 2019}, {"title": "Contextual Decision Processes with Low Bellman Rank are PAC-Learnable", "authors": ["Nan Jiang", "Akshay Krishnamurthy", "Alekh Agarwal", "John Langford", "Robert E. Schapire"], "conference": "ICML-17", "year": 2017}, {"title": "Doubly Robust Off-policy Value Evaluation for Reinforcement Learning", "authors": ["Nan Jiang", "Lihong Li"], "conference": "ICML-16", "year": 2016}, {"title": "The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation", "authors": ["P. Amortila", "N. Jiang", "C. Szepesv\u00e1ri"], "conference": "ICML-23", "year": 2023}, {"title": "Adversarial Model for Offline Reinforcement Learning", "authors": ["M. Bhardwaj", "T. Xie", "B. Boots", "N. Jiang", "C. Cheng"], "conference": "NeurIPS-23", "year": 2023}, {"title": "Offline reinforcement learning under value and density-ratio realizability: The power of gaps", "authors": ["J. Chen", "N. Jiang"], "conference": "UAI-22", "year": 2022}, {"title": "Beyond the Return: Off-policy Function Estimation under User-specified Error-measuring Distributions", "authors": ["A. Huang", "J. Chen", "N. Jiang"], "conference": "NeurIPS-22", "year": 2022}, {"title": "Tiered Reinforcement Learning: Pessimism in the Face of Uncertainty and Constant Regret", "authors": ["J. Huang", "L. Zhao", "T. Qin", "W. Chen", "N. Jiang", "T.-Y. Liu"], "conference": "NeurIPS-22", "year": 2022}, {"title": "A Minimax Learning Approach to Off-Policy Evaluation in Confounded Partially Observable Markov Decision Processes", "authors": ["C. Shi", "M. Uehara", "J. Huang", "N. Jiang"], "conference": "ICML-22", "year": 2022}, {"title": "THE ROLE OF COVERAGE IN ONLINE REINFORCEMENT LEARNING", "authors": ["T. Xie", "D. Foster", "Y. Bai", "N. Jiang", "S. Kakade"], "conference": "ICLR-23", "year": 2023}, {"title": "Offline Learning in Markov Games with General Function Approximation", "authors": ["Y. Zhang", "Y. Bai", "N. Jiang"], "conference": "ICML-23", "year": 2023}, {"title": "Offline Reinforcement Learning with Realizability and Single-policy Concentrability", "authors": ["W. Zhan", "B. Huang", "A. Huang", "N. Jiang", "J. Lee"], "conference": "COLT-22", "year": 2022}], "teachingHonors": [{"honor": "Teachers Ranked as Excellent (Excellent)", "year": 2023}, {"honor": "Teachers Ranked as Excellent (Excellent)", "year": 2022}, {"honor": "Engineering Council Outstanding Advisor Award", "year": 2022}, {"honor": "Teachers Ranked as Excellent (Excellent)", "year": 2021}, {"honor": "Teachers Ranked as Excellent (Outstanding)", "year": 2020}, {"honor": "Teachers Ranked as Excellent (Excellent)", "year": 2019}, {"honor": "Teachers Ranked as Excellent (Excellent)", "year": 2018}], "researchHonors": [{"honor": "Google Research Scholar Award", "organization": "Google", "year": 2024}, {"honor": "Sloan Research Fellowship", "organization": "Alfred P. Sloan Foundation", "year": 2024}, {"honor": "ICML 2022 Outstanding Paper Runner Up", "organization": "ICML", "year": 2022}, {"honor": "NSF CAREER Award", "organization": "National Science Foundation", "year": 2022}, {"honor": "AAMAS 2015 Best Paper Award", "organization": "AAMAS", "year": 2015}], "coursesTaught": [{"code": "CS 443", "title": "Reinforcement Learning"}, {"code": "CS 542", "title": "Stat Reinforcement Learning"}, {"code": "CS 598 NJ", "title": "Statistical Reinforcement Learning"}, {"code": "CS 598 NJ", "title": "Statistical Reinforcement Learning"}]}