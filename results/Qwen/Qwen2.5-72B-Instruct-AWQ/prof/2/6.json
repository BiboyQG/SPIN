{"fullname": "Minjia Zhang", "title": "Assistant Professor (CS)", "contact": {"phone": "", "email": "minjiaz@illinois.edu"}, "office": "Thomas M. Siebel Center for Computer Science 4106, 201 North Goodwin Avenue MC 258 Urbana, IL 61801", "education": [{"degree": "Ph.D.", "field": "Computer Science and Engineering", "institution": "Ohio State University, USA", "year": 2016}], "biography": "I am an assistant professor (tenure-track) at the Grainger College of Engineering Computer Science of the University of Illinois Urbana-Champaign. I am affiliated with the Department of Electrical and Computer Engineering and NCSA at UIUC. Prior to my appointment at UIUC, I had seven years at Microsoft Research Redmond and WebXT division as a Principal Researcher and technical lead. My research works have been published in major venues, including system and high-performance computing conferences (e.g., ASPLOS, NSDI, USENIX ATC, SC), and top-tier machine learning conferences (e.g., ICML, NeurIPS, ICLR).", "professionalHighlights": [{"position": "Tenure-Track Assistant Professor", "organization": "Computer Science Department, Univ. of Illinois", "yearStart": 2023, "yearEnd": null}], "researchStatement": "My research focuses on efficient and scalable machine learning systems, parallel computing, AI efficiency, model compression, and natural language processing. I aim to develop systems and algorithms that can handle large-scale deep learning training and inference on parallel and distributed hardware, while also ensuring cost-effectiveness and performance.", "researchInterests": [{"area": "Machine Learning Systems", "description": "Developing efficient and scalable systems for training and inference of large-scale deep learning models."}, {"area": "Parallel Computing", "description": "Designing and optimizing parallel algorithms and runtime systems for high-performance computing."}, {"area": "AI Efficiency", "description": "Improving the efficiency of AI systems through data and model optimization techniques."}, {"area": "Model Compression", "description": "Reducing the size and computational cost of deep neural networks without sacrificing performance."}, {"area": "Natural Language Processing", "description": "Advancing the state-of-the-art in natural language processing and understanding."}], "researchAreas": ["Machine Learning Systems", "Parallel Computing", "AI Efficiency", "Model Compression", "Natural Language Processing"], "publications": [{"title": "DeepSpeed-Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models", "authors": ["Sam Ade Jacobs", "Masahiro Tanaka", "Chengming Zhang", "Minjia Zhang", "Reza Yazdani Aminabadi", "Shuaiwen Leon Song", "Samyam Rajbhandari", "Yuxiong He"], "conference": "PODC 2024", "year": 2024}, {"title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs", "authors": ["Suyu Ge", "Yunan Zhang", "Liyuan Liu", "Minjia Zhang", "Jiawei Han", "Jianfeng Gao"], "conference": "ICLR 2024 (Oral, Honorable Mention of the Outstanding Paper Awards)", "year": 2024}, {"title": "Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale", "authors": ["Samyam Rajbhandari", "Conglong Li", "Zhewei Yao", "Minjia Zhang", "Reza Yazdani Aminabadi", "Ammar Ahmad Awan", "Jeff Rasley", "Yuxiong He"], "conference": "ICML 2022", "year": 2022}, {"title": "Extreme Compression for Pre-trained Transformers Made Simple and Efficient", "authors": ["Xiaoxia Wu", "Zhewei Yao", "Minjia Zhang", "Conglong Li", "Yuxiong He"], "conference": "NeurIPS 2022 (Oral)", "year": 2022}, {"title": "ZeRO-Offload: Democratizing Billion-Scale Model Training", "authors": ["Jie Ren", "Samyam Rajbhandari", "Reza Yazdani Aminabadi", "Olatunji Ruwase", "Shuangyan Yang", "Minjia Zhang", "Dong Li", "Yuxiong He"], "conference": "USENIX ATC 2021", "year": 2021}, {"title": "Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training", "authors": ["Xinyu Lian", "Sam Ade Jacobs", "Lev Kurilenko", "Masahiro Tanaka", "Stas Bekman", "Olatunji Ruwase", "Minjia Zhang"], "conference": "arXiv preprint arXiv:2406.18820", "year": 2024}], "teachingHonors": [], "researchHonors": [{"honor": "Distinguished Paper Award", "organization": "OOPSLA 2015", "year": 2015}, {"honor": "Distinguished Artifact Award", "organization": "OOPSLA 2015", "year": 2015}, {"honor": "Microsoft Excellence Awards", "organization": "Microsoft", "year": 2019}, {"honor": "Honorable Mention of the ICLR 2024 Outstanding Paper Award", "organization": "ICLR 2024", "year": 2024}], "coursesTaught": [{"code": "CS 598 AIE", "title": "AI Efficiency: Sys. & Algor."}]}