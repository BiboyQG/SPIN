{"fullname": "Minjia Zhang", "title": "Assistant Professor", "contact": {"phone": "", "email": "minjiaz@illinois.edu"}, "office": "Thomas M. Siebel Center for Computer Science 4106, 201 North Goodwin Avenue MC 258 Urbana, IL 61801", "education": [{"degree": "Ph.D.", "field": "Computer Science and Engineering", "institution": "Ohio State University, USA", "year": 2016}], "biography": "I am an assistant professor (tenure-track) at the Grainger College of Engineering Computer Science of the University of Illinois Urbana-Champaign. I am affiliated with the Department of Electrical and Computer Engineering and NCSA at UIUC. Prior to my appointment at UIUC, I had seven years at Microsoft Research Redmond and WebXT division as a Principal Researcher and technical lead. My research works have been published in major venues, including system and high-performance computing conferences (e.g., ASPLOS, NSDI, USENIX ATC, SC), and top-tier machine learning conferences (e.g., ICML, NeurIPS, ICLR). Several of my work has been applied to Microsoft systems and products, such as Bing, Ads, Azure SQL, Windows, etc., leading to significant latency improvement and cost reduction. I was an early member of DeepSpeed, an open-source deep learning optimization library that makes training and inference DL models easy, efficient, and effective. DeepSpeed has enabled the training of some of the largest language models in the world, such as Megatron-Turing 530B. It has been widely adopted by both the industry and academia and has become a common backend for various popular DL frameworks such as HuggingFace, PyTorch Lightning, Fairscale, etc. I was also the co-chair of the engineering/scaling group of the BigScience project, contributing to the training of the BLOOM 176B model, which was the world's largest open multilingual language model. Before DeepSpeed, I drove the DeepCPU project at Microsoft, a DL inference optimization library that brought order-of-magnitude latency and cost reduction to mission-critical production DL models.", "professionalHighlights": [{"position": "Tenure-Track Assistant Professor", "organization": "Computer Science Department, Univ. of Illinois", "yearStart": 2016, "yearEnd": null}], "researchStatement": "My research focuses on developing efficient and scalable systems and algorithms for large-scale machine learning and deep learning. I am particularly interested in optimizing the training and inference of deep neural networks on parallel, distributed, and heterogeneous hardware. My work aims to reduce computational costs, improve performance, and enable the deployment of large-scale models in real-world applications.", "researchInterests": [{"area": "Machine Learning Systems", "description": "Developing efficient and scalable systems for training and inference of deep neural networks."}, {"area": "Parallel Computing", "description": "Optimizing parallel and distributed computing for large-scale machine learning tasks."}, {"area": "AI Efficiency", "description": "Improving the efficiency of AI models through algorithmic and system-level optimizations."}, {"area": "Model Compression", "description": "Reducing the size and computational requirements of deep neural networks without sacrificing performance."}, {"area": "Natural Language Processing", "description": "Advancing the state-of-the-art in natural language processing through efficient and scalable systems."}], "researchAreas": ["Machine Learning Systems", "Parallel Computing", "AI Efficiency", "Model Compression", "Natural Language Processing"], "publications": [{"title": "DeepSpeed-Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models", "authors": ["Sam Ade Jacobs", "Masahiro Tanaka", "Chengming Zhang", "Minjia Zhang", "Reza Yazdani Aminabadi", "Shuaiwen Leon Song", "Samyam Rajbhandari", "Yuxiong He"], "conference": "PODC 2024", "year": 2024}, {"title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs", "authors": ["Suyu Ge", "Yunan Zhang", "Liyuan Liu", "Minjia Zhang", "Jiawei Han", "Jianfeng Gao"], "conference": "ICLR 2024 (Oral, Honorable Mention of the Outstanding Paper Awards)", "year": 2024}, {"title": "Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale", "authors": ["Samyam Rajbhandari", "Conglong Li", "Zhewei Yao", "Minjia Zhang", "Reza Yazdani Aminabadi", "Ammar Ahmad Awan", "Jeff Rasley", "Yuxiong He"], "conference": "ICML 2022", "year": 2022}, {"title": "Extreme Compression for Pre-trained Transformers Made Simple and Efficient", "authors": ["Xiaoxia Wu", "Zhewei Yao", "Minjia Zhang", "Conglong Li", "Yuxiong He"], "conference": "NeurIPS 2022 (Oral)", "year": 2022}, {"title": "ZeRO-Offload: Democratizing Billion-Scale Model Training", "authors": ["Jie Ren", "Samyam Rajbhandari", "Reza Yazdani Aminabadi", "Olatunji Ruwase", "Shuangyan Yang", "Minjia Zhang", "Dong Li", "Yuxiong He"], "conference": "USENIX ATC 2021", "year": 2021}], "teachingHonors": [{"honor": "", "year": 0}], "researchHonors": [{"honor": "Distinguished Paper Award and Distinguished Artifact Award in OOPSLA 2015", "organization": "ACM", "year": 2015}, {"honor": "Honorable Mention of the ICLR 2024 Outstanding Paper Award", "organization": "ICLR", "year": 2024}], "coursesTaught": [{"code": "CS 598 AIE", "title": "AI Efficiency: Sys. & Algor."}]}