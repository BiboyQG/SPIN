{"fullname": "Bo Li", "title": "Assistant Professor", "contact": {"phone": "(217) 300-8141", "email": "lbo@illinois.edu"}, "office": "4310 Siebel Center for Comp Sci", "education": [{"degree": "", "field": "", "institution": "", "year": 0}], "biography": "I am on the advisory board of the Center for Artificial Intelligence Innovation (CAII) at Illinois, and I am a member of the Information Trust Institute (ITI). I am also affiliated with several research centers aiming to broaden the research collaboration and bridge different communities, such as the Advanced Digital Science Center (ADSC), the Center for Cognitive Computing Systems Research (C3SR), and the Quantum Information Science and Technology Center (IQUIST). I also serve in the Accelerated Learning and Engineering Research Training (ALERT) program. My research focuses on trustworthy machine learning, with an emphasis on robustness, privacy, generalization, and their interconnections. The long-term goal for our group, Secure learning lab (SL2), is to make machine learning systems robust, private, and generalizable with guarantees for different real-world applications.", "professionalHighlights": [{"position": "Assistant Professor", "organization": "Siebel School of Computing and Data Science, Illinois", "yearStart": 0, "yearEnd": null}], "researchStatement": "My research focuses on making machine learning algorithms more robust, private, efficient, and interpretable with guarantees. Through my Secure Learning Lab, I work with students to explore different adversarial attacks and defenses to benefit the application of machine learning techniques through computer vision, natural language processing, audio recognition, autonomous vehicles, and medical healthcare. If we cannot evaluate or improve safety and robustness, that will lead to severe consequences eventually. Therefore, I remain really interested in first trying to attack these systems to see what happens. Then we try to improve the robustness within primary applications like autonomous driving and medical healthcare \u2013 to really help those safety critical areas. Additionally, I am involved in several projects funded by the C3.ai Digital Transformation Institute (DTI) focused on using AI to harden information security and secure critical infrastructure, including projects on protecting critical infrastructures against evolving insider threats, resilient distributed cybersecurity learning systems, and robust and scalable forensics for deep neural networks.", "researchInterests": [{"area": "Information Theory", "description": ""}, {"area": "Game Theory", "description": ""}, {"area": "Privacy", "description": ""}, {"area": "Trustworthy Machine Learning", "description": ""}, {"area": "Security", "description": ""}, {"area": "Artificial Intelligence", "description": ""}], "researchAreas": ["Artificial Intelligence", "Security and Privacy"], "publications": [{"title": "Robust physical-world attacks on deep learning visual classification", "authors": ["K Eykholt", "I Evtimov", "E Fernandes", "B Li", "A Rahmati", "C Xiao", "A Prakash", "T Kohno", "D Song"], "conference": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "year": 2018}, {"title": "Targeted backdoor attacks on deep learning systems using data poisoning", "authors": ["X Chen", "C Liu", "B Li", "K Lu", "D Song"], "conference": "arXiv preprint arXiv:1712.05526", "year": 2017}, {"title": "Generating adversarial examples with adversarial networks", "authors": ["C Xiao", "B Li", "JY Zhu", "W He", "M Liu", "D Song"], "conference": "arXiv preprint arXiv:1801.02610", "year": 2018}, {"title": "Manipulating machine learning: Poisoning attacks and countermeasures for regression learning", "authors": ["M Jagielski", "A Oprea", "B Biggio", "C Liu", "C Nita-Rotaru", "B Li"], "conference": "2018 IEEE Symposium on Security and Privacy (SP)", "year": 2018}, {"title": "Characterizing adversarial subspaces using local intrinsic dimensionality", "authors": ["X Ma", "B Li", "Y Wang", "SM Erfani", "S Wijewickrema", "G Schoenebeck", "D Song", "C Leckie", "J Bailey"], "conference": "arXiv preprint arXiv:1801.02613", "year": 2018}, {"title": "Textbugger: Generating adversarial text against real-world applications", "authors": ["J Li", "S Ji", "T Du", "B Li", "T Wang"], "conference": "arXiv preprint arXiv:1812.05271", "year": 2018}, {"title": "Deepgauge: Multi-granularity testing criteria for deep learning systems", "authors": ["L Ma", "F Juefei-Xu", "F Zhang", "J Sun", "M Xue", "B Li", "C Chen", "T Su", "L Li", "Y Liu", "J Zhao"], "conference": "Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering", "year": 2018}, {"title": "DBA: Distributed Backdoor Attacks against Federated Learning", "authors": ["C Xie", "K Huang", "PY Chen", "B Li"], "conference": "International Conference on Learning Representations", "year": 2019}, {"title": "Spatially transformed adversarial examples", "authors": ["C Xiao", "JY Zhu", "B Li", "W He", "M Liu", "D Song"], "conference": "arXiv preprint arXiv:1801.02612", "year": 2018}, {"title": "Physical adversarial examples for object detectors", "authors": ["D Song", "K Eykholt", "I Evtimov", "E Fernandes", "B Li", "A Rahmati", "F Tramer", "C Xiao", "T Kohno", "D Song"], "conference": "12th USENIX Workshop on Offensive Technologies (WOOT 18)", "year": 2018}, {"title": "The secret revealer: generative model-inversion attacks against deep neural networks", "authors": ["Y Zhang", "R Jia", "H Pei", "W Wang", "B Li", "D Song"], "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2020}, {"title": "Towards efficient data valuation based on the shapley value", "authors": ["R Jia", "D Dao", "B Wang", "FA Hubis", "N Hynes", "NM G\u00fcrel", "B Li", "C Zhang", "D Song"], "conference": "The 22nd International Conference on Artificial Intelligence and Statistics", "year": 2019}, {"title": "Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks", "authors": ["Y Li", "X Lyu", "N Koren", "L Lyu", "B Li", "X Ma"], "conference": "arXiv preprint arXiv:2101.05930", "year": 2021}, {"title": "Deephunter: A coverage-guided fuzz testing framework for deep neural networks", "authors": ["X Xie", "L Ma", "F Juefei-Xu", "M Xue", "H Chen", "Y Liu", "J Zhao", "B Li", "J Yin", "S See"], "conference": "Proceedings of the 28th ACM SIGSOFT International Symposium on Software Engineering", "year": 2019}, {"title": "Deepmutation: Mutation testing of deep learning systems", "authors": ["L Ma", "F Zhang", "J Sun", "M Xue", "B Li", "F Juefei-Xu", "C Xie", "L Li", "Y Liu", "J Zhao", "S See"], "conference": "2018 IEEE 29th International Symposium on Software Reliability Engineering", "year": 2018}, {"title": "Data poisoning attacks on factorization-based collaborative filtering", "authors": ["B Li", "Y Wang", "A Singh", "Y Vorobeychik"], "conference": "Advances in Neural Information Processing Systems", "year": 2016}, {"title": "Towards stable and efficient training of verifiably robust neural networks", "authors": ["H Zhang", "H Chen", "C Xiao", "S Gowal", "R Stanforth", "B Li", "D Boning", "CJ Hsieh"], "conference": "arXiv preprint arXiv:1906.06316", "year": 2019}, {"title": "Adversarial attack and defense on graph data: A survey", "authors": ["L Sun", "Y Dou", "C Yang", "K Zhang", "J Wang", "SY Philip", "L He", "B Li"], "conference": "IEEE Transactions on Knowledge and Data Engineering", "year": 2022}], "teachingHonors": [{"honor": "Teachers Ranked as Excellent Award", "year": 2021}], "researchHonors": [{"honor": "IJCAI Computers and Thought Award", "organization": "International Joint Conference on Artificial Intelligence", "year": 2022}, {"honor": "AI's 10 to Watch", "organization": "", "year": 2022}, {"honor": "Google Faculty Research Award", "organization": "", "year": 2022}, {"honor": "First prize in the International Verification Neural Networks Competition (VNN-COMP'22)", "organization": "", "year": 2022}, {"honor": "Dean's Award for Excellence in Research", "organization": "", "year": 2022}, {"honor": "C.W. Gear Outstanding Junior Faculty Award", "organization": "", "year": 2021}, {"honor": "Facebook Research Award", "organization": "", "year": 2021}, {"honor": "Best Paper Award in EWSN", "organization": "", "year": 2021}, {"honor": "Intel's 2020 Rising Star Faculty Award", "organization": "", "year": 2020}, {"honor": "Amazon Research Award", "organization": "", "year": 2020}, {"honor": "NSF CAREER Award", "organization": "", "year": 2020}, {"honor": "MIT Technology Review 35 Innovators Under 35", "organization": "", "year": 2020}, {"honor": "Facebook Research Award", "organization": "", "year": 2019}, {"honor": "Amazon Research Award", "organization": "", "year": 2019}, {"honor": "IBM Research Award", "organization": "", "year": 2019}, {"honor": "Q4 AWS Machine Learning Research Awards", "organization": "", "year": 2019}], "coursesTaught": [{"code": "CS 307", "title": "Model & Learning in Data Sci"}, {"code": "CS 442 (CS 498 LB1, CS 498 LB2)", "title": "Trustworthy Machine Learning"}, {"code": "CS 562", "title": "Adv Topics in Sec, Priv and ML"}, {"code": "CS 598 BL", "title": "Adversarial Machine Learning"}]}