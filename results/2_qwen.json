{
    "fullname": "Minjia Zhang",
    "title": "Assistant Professor",
    "contact": {
        "phone": "",
        "email": "minjiaz@illinois.edu"
    },
    "office": "Thomas M. Siebel Center for Computer Science 4106, 201 North Goodwin Avenue MC 258 Urbana, IL 61801",
    "education": [
        {
            "degree": "Ph.D.",
            "field": "Computer Science and Engineering",
            "institution": "Ohio State University, USA",
            "year": 2016
        }
    ],
    "biography": "Minjia Zhang is an assistant professor (tenure-track) at the Grainger College of Engineering Computer Science of the University of Illinois Urbana-Champaign. He is affiliated with the Department of Electrical and Computer Engineering and the Center for Artificial Intelligence Innovation (CAII) at the National Center for Supercomputing Applications (NCSA) at UIUC. Prior to his appointment at UIUC, he had a seven-year tenure at Microsoft Research Redmond and WebXT division as a Principal Researcher and technical lead. His research focuses on efficient machine learning systems, effective efficiency algorithms, and large-scale DL/AI applications.",
    "professionalHighlights": [
        {
            "position": "Tenure-Track Assistant Professor",
            "organization": "Computer Science Department, Univ. of Illinois",
            "yearStart": 2023,
            "yearEnd": null
        }
    ],
    "researchStatement": "My research interests include efficient machine learning systems, effective efficiency algorithms, and large-scale DL/AI applications. I am particularly interested in training and inference on parallel/distributed/heterogeneous hardware, model compression, data efficiency, parameter-efficient tuning, and large-scale DL/AI applications such as RAG, Image/Video Generation, VLM, DLRM, etc.",
    "researchInterests": [
        {
            "area": "Machine Learning Systems",
            "description": "Training and inference on parallel/distributed/heterogeneous hardware"
        },
        {
            "area": "Parallel Computing",
            "description": "Building efficient and scalable systems for parallel programs"
        },
        {
            "area": "AI Efficiency",
            "description": "Model compression, data efficiency, parameter-efficient tuning"
        },
        {
            "area": "Model Compression",
            "description": "Smaller, faster, and cheaper DNNs"
        },
        {
            "area": "Natural Language Processing",
            "description": "Large-scale DL/AI applications such as RAG, Image/Video Generation, VLM, DLRM, etc"
        }
    ],
    "researchAreas": [
        "Machine Learning Systems",
        "Parallel Computing",
        "AI Efficiency",
        "Model Compression",
        "Natural Language Processing"
    ],
    "publications": [
        {
            "title": "DeepSpeed-Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
            "authors": [
                "Sam Ade Jacobs",
                "Masahiro Tanaka",
                "Chengming Zhang",
                "Minjia Zhang",
                "Reza Yazdani Aminabadi",
                "Shuaiwen Leon Song",
                "Samyam Rajbhandari",
                "Yuxiong He"
            ],
            "conference": "PODC 2024",
            "year": 2024
        },
        {
            "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
            "authors": [
                "Suyu Ge",
                "Yunan Zhang",
                "Liyuan Liu",
                "Minjia Zhang",
                "Jiawei Han",
                "Jianfeng Gao"
            ],
            "conference": "ICLR 2024 (Oral, Honorable Mention of the Outstanding Paper Awards)",
            "year": 2024
        },
        {
            "title": "Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale",
            "authors": [
                "Samyam Rajbhandari",
                "Conglong Li",
                "Zhewei Yao",
                "Minjia Zhang",
                "Reza Yazdani Aminabadi",
                "Ammar Ahmad Awan",
                "Jeff Rasley",
                "Yuxiong He"
            ],
            "conference": "ICML 2022",
            "year": 2022
        },
        {
            "title": "Extreme Compression for Pre-trained Transformers Made Simple and Efficient",
            "authors": [
                "Xiaoxia Wu",
                "Zhewei Yao",
                "Minjia Zhang",
                "Conglong Li",
                "Yuxiong He"
            ],
            "conference": "NeurIPS 2022 (Oral)",
            "year": 2022
        },
        {
            "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training",
            "authors": [
                "Jie Ren",
                "Samyam Rajbhandari",
                "Reza Yazdani Aminabadi",
                "Olatunji Ruwase",
                "Shuangyan Yang",
                "Minjia Zhang",
                "Dong Li",
                "Yuxiong He"
            ],
            "conference": "USENIX ATC 2021",
            "year": 2021
        }
    ],
    "teachingHonors": [],
    "researchHonors": [],
    "coursesTaught": [
        {
            "code": "CS 598 AIE",
            "title": "AI Efficiency: Sys. & Algor."
        }
    ]
}