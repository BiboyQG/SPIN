{"fullname": "Minjia Zhang", "title": "Assistant Professor", "contact": {"phone": "", "email": "minjiaz@illinois.edu"}, "office": "Thomas M. Siebel Center for Computer Science 4106, 201 North Goodwin Avenue MC 258 Urbana, IL 61801", "education": [{"degree": "Ph.D.", "field": "Computer Science", "institution": "Ohio State University", "year": 2016}], "biography": "I am an assistant professor (tenure-track) at the Grainger College of Engineering Computer Science of the University of Illinois Urbana-Champaign. I am affiliated with the Department of Electrical and Computer Engineering and NCSA at UIUC. Prior to my appointment at UIUC, I had wonderful seven years at Microsoft Research Redmond and WebXT division as a Principal Researcher and technical lead.", "professionalHighlights": [{"position": "Assistant Professor", "organization": "Department of Computer Science, University of Illinois at Urbana-Champaign", "yearStart": 2023, "yearEnd": null}, {"position": "Affiliate Professor", "organization": "Department of ECE, University of Illinois at Urbana-Champaign", "yearStart": 2023, "yearEnd": null}, {"position": "Affiliate Professor", "organization": "Center for Artificial Intelligence Innovation (CAII) at National Center for Supercomputing Applications (NCSA)", "yearStart": 2023, "yearEnd": null}, {"position": "Principal Researcher and Technical Lead", "organization": "Microsoft Research Redmond and WebXT division", "yearStart": 2016, "yearEnd": 2023}], "researchStatement": "My research focuses on efficient machine learning systems, parallel computing, AI efficiency, model compression, and natural language processing.", "researchInterests": [{"area": "Efficient Machine Learning Systems", "description": "Training and inference on parallel/distributed/heterogeneous hardware"}, {"area": "Parallel Computing", "description": "Scalable runtime and efficient algorithms for parallel programs"}, {"area": "AI Efficiency", "description": "Model compression, data efficiency, and parameter-efficient tuning"}, {"area": "Model Compression", "description": "Smaller, faster, and cheaper DNN models"}, {"area": "Natural Language Processing", "description": "Language models, text generation, and language understanding"}], "researchAreas": ["Efficient Machine Learning Systems", "Parallel Computing", "AI Efficiency", "Model Compression", "Natural Language Processing"], "publications": [{"title": "DeepSpeed-Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models", "authors": ["Sam Ade Jacobs", "Masahiro Tanaka", "Chengming Zhang", "Minjia Zhang", "Reza Yazdani Aminabadi", "Shuaiwen Leon Song", "Samyam Rajbhandari", "Yuxiong He"], "conference": "PODC 2024", "year": 2024}, {"title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs", "authors": ["Suyu Ge", "Yunan Zhang", "Liyuan Liu", "Minjia Zhang", "Jiawei Han", "Jianfeng Gao"], "conference": "ICLR 2024", "year": 2024}], "teachingHonors": [{"honor": "Outstanding Teacher Award", "year": 2022}], "researchHonors": [{"honor": "Distinguished Paper Award", "organization": "OOPSLA 2015", "year": 2015}, {"honor": "Distinguished Artifact Award", "organization": "OOPSLA 2015", "year": 2015}], "coursesTaught": [{"code": "CS 598 AIE", "title": "AI Efficiency: Systems and Algorithms"}]}