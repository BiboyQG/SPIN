{"fullname": "Bo Li", "title": "Assistant Professor", "contact": {"phone": "(217) 300-8141", "email": "lbo@illinois.edu"}, "office": "4310 Siebel Center for Comp Sci", "education": [{"degree": "", "field": "", "institution": "University of Illinois at Urbana-Champaign", "year": 0}], "biography": "Bo Li is an Assistant Professor of Computer Science at the University of Illinois at Urbana-Champaign. Her research focuses on trustworthy machine learning, information theory, game theory, artificial intelligence, computer security and privacy. She joined the Illinois CS faculty in 2018 and is also affiliated with electrical and computer engineering. Li has received several awards, including the 2022 Sloan Research Fellowship, the National Science Foundation CAREER Award, Intel's 2020 Rising Star Faculty Award, and the 2019 and 2020 Q4 Amazon Web Services Machine Learning Research Awards. By devising new ways to fool AI, she is making it safer. A few years ago, Bo Li and her colleagues placed small black-and-white stickers on a stop sign in a graffiti-like pattern that looked random to human eyes and did not obscure the sign\u2019s clear lettering. Yet the arrangement was deliberately designed so that if an autonomous vehicle approached, the neural networks powering its vision system would misread the stop sign as one posting a speed limit of 45 mph. I am on the advisory board of the Center for Artificial Intelligence Innovation (CAII) at Illinois, and I am a member of the Information Trust Institute (ITI). I am also affiliated with several research centers aiming to broaden the research collaboration and bridge different communities, such as the Advanced Digital Science Center (ADSC), the Center for Cognitive Computing Systems Research (C3SR), and the Quantum Information Science and Technology Center (IQUIST). I also serve in the Accelerated Learning and Engineering Research Training (ALERT) program.", "professionalHighlights": [{"position": "Assistant Professor", "organization": "University of Illinois at Urbana-Champaign", "yearStart": 2018, "yearEnd": null}], "researchStatement": "My research focuses on trustworthy machine learning, with an emphasis on robustness, privacy, generalization, and their interconnections. We believe that closing today's trustworthiness gap in ML requires us to tackle these grappled problems in a holistic framework, driven by fundamental research focusing on not only each problem but also their underlying interactions. The long-term goal for our group, Secure learning lab (SL2), is to make machine learning systems robust, private, and generalizable with guarantees for different real-world applications. We have worked on exploring different types of adversarial attacks, including evasion and poisoning attacks in digital and physical worlds, under various constraints. We have developed and will continue to explore robust learning systems based on game-theoretic analysis, knowledge-enabled logical reasoning, and properties of learning tasks. Our work directly benefits applications such as computer vision, natural language processing, safe autonomous driving, and trustworthy federated learning systems. Additionally, my colleagues Han Zhao and Tong Zhang have received a $800,000 NSF award for their project SLES: Monitoring, Improving, and Certifying Safe Foundation Models, which aims to develop a set of quantifiable safety measures that can be computed automatically from the model\u2019s output to measure and mitigate the inconsistency and hallucination of LLMs based on the theory of optimal transport. I am also involved in the newly formed NSF AI Institute for Agent-based Cyber Threat Intelligence and OperatioN (ACTION), which aims to revolutionize protection for mission-critical systems against sophisticated cyber threats. My work also includes research awards from tech companies such as Amazon, Meta, Google, Intel, MSR, eBay, and IBM, and best paper awards at several top machine learning and security conferences.", "researchInterests": [{"area": "Information Theory", "description": ""}, {"area": "Game Theory", "description": ""}, {"area": "Privacy", "description": ""}, {"area": "Trustworthy Machine Learning", "description": ""}, {"area": "Security", "description": ""}, {"area": "Artificial Intelligence", "description": ""}], "researchAreas": ["Artificial Intelligence", "Security and Privacy"], "publications": [{"title": "SoK: Certified Robustness for Deep Neural Networks", "authors": ["Linyi Li", "Tao Xie", "Bo Li"], "conference": "44th IEEE Symposium on Security and Privacy, SP 2023, San Francisco, CA, USA, 22-26 May 2023", "year": 2023}, {"title": "Robust physical-world attacks on deep learning visual classification", "authors": ["K Eykholt", "I Evtimov", "E Fernandes", "B Li", "A Rahmati", "C Xiao", "A Prakash"], "conference": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "year": 2018}, {"title": "Targeted backdoor attacks on deep learning systems using data poisoning", "authors": ["X Chen", "C Liu", "B Li", "K Lu", "D Song"], "conference": "arXiv preprint arXiv:1712.05526", "year": 2017}, {"title": "Generating adversarial examples with adversarial networks", "authors": ["C Xiao", "B Li", "JY Zhu", "W He", "M Liu", "D Song"], "conference": "arXiv preprint arXiv:1801.02610", "year": 2018}, {"title": "Manipulating machine learning: Poisoning attacks and countermeasures for regression learning", "authors": ["M Jagielski", "A Oprea", "B Biggio", "C Liu", "C Nita-Rotaru", "B Li"], "conference": "2018 IEEE Symposium on Security and Privacy (SP)", "year": 2018}, {"title": "Characterizing adversarial subspaces using local intrinsic dimensionality", "authors": ["X Ma", "B Li", "Y Wang", "SM Erfani", "S Wijewickrema", "G Schoenebeck", "D Song"], "conference": "arXiv preprint arXiv:1801.02613", "year": 2018}, {"title": "Textbugger: Generating adversarial text against real-world applications", "authors": ["J Li", "S Ji", "T Du", "B Li", "T Wang"], "conference": "arXiv preprint arXiv:1812.05271", "year": 2018}, {"title": "Deepgauge: Multi-granularity testing criteria for deep learning systems", "authors": ["L Ma", "F Juefei-Xu", "F Zhang", "J Sun", "M Xue", "B Li", "C Chen", "T Su", "L Li", "Y Liu", "J Zhao"], "conference": "Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering", "year": 2018}, {"title": "DBA: Distributed Backdoor Attacks against Federated Learning", "authors": ["C Xie", "K Huang", "PY Chen", "B Li"], "conference": "International Conference on Learning Representations", "year": 2019}, {"title": "Spatially transformed adversarial examples", "authors": ["C Xiao", "JY Zhu", "B Li", "W He", "M Liu", "D Song"], "conference": "arXiv preprint arXiv:1801.02612", "year": 2018}, {"title": "Physical adversarial examples for object detectors", "authors": ["D Song", "K Eykholt", "I Evtimov", "E Fernandes", "B Li", "A Rahmati", "F Tramer"], "conference": "12th {USENIX} Workshop on Offensive Technologies ({WOOT} 18)", "year": 2018}, {"title": "The secret revealer: generative model-inversion attacks against deep neural networks", "authors": ["Y Zhang", "R Jia", "H Pei", "W Wang", "B Li", "D Song"], "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2020}, {"title": "Towards efficient data valuation based on the shapley value", "authors": ["R Jia", "D Dao", "B Wang", "FA Hubis", "N Hynes", "NM G\u00fcrel", "B Li", "C Zhang"], "conference": "The 22nd International Conference on Artificial Intelligence and Statistics", "year": 2019}, {"title": "Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks", "authors": ["Y Li", "X Lyu", "N Koren", "L Lyu", "B Li", "X Maar"], "conference": "arXiv preprint arXiv:2101.05930", "year": 2021}, {"title": "Deephunter: A coverage-guided fuzz testing framework for deep neural networks", "authors": ["X Xie", "L Ma", "F Juefei-Xu", "M Xue", "H Chen", "Y Liu", "J Zhao", "B Li", "J Yin", "S See"], "conference": "Proceedings of the 28th ACM SIGSOFT International Symposium on Software Engineering", "year": 2019}, {"title": "Deepmutation: Mutation testing of deep learning systems", "authors": ["L Ma", "F Zhang", "J Sun", "M Xue", "B Li", "F Juefei-Xu", "C Xie", "L Li", "Y Liu", "J Zhao"], "conference": "2018 IEEE 29th International Symposium on Software Reliability Engineering", "year": 2018}, {"title": "Data poisoning attacks on factorization-based collaborative filtering", "authors": ["B Li", "Y Wang", "A Singh", "Y Vorobeychik"], "conference": "Advances in Neural Information Processing Systems 29", "year": 2016}, {"title": "Adversarial attack and defense on graph data: A survey", "authors": ["L Sun", "Y Dou", "C Yang", "K Zhang", "J Wang", "SY Philip", "L He", "B Li"], "conference": "IEEE Transactions on Knowledge and Data Engineering", "year": 2022}, {"title": "Towards stable and efficient training of verifiably robust neural networks", "authors": ["H Zhang", "H Chen", "C Xiao", "S Gowal", "R Stanforth", "B Li", "D Boning", "C-J Hsieh"], "conference": "arXiv preprint arXiv:1906.06316", "year": 2019}], "teachingHonors": [{"honor": "Teachers Ranked as Excellent Award", "year": 2021}], "researchHonors": [{"honor": "IJCAI Computers and Thought Award", "organization": "", "year": 2022}, {"honor": "AI's 10 to Watch", "organization": "", "year": 2022}, {"honor": "Google Faculty Research Award", "organization": "", "year": 2022}, {"honor": "First prize in the International Verification Neural Networks Competition (VNN-COMP'22)", "organization": "", "year": 2022}, {"honor": "Dean's Award for Excellence in Research", "organization": "", "year": 2022}, {"honor": "C.W. Gear Outstanding Junior Faculty Award", "organization": "", "year": 2021}, {"honor": "Facebook Research Award", "organization": "", "year": 2021}, {"honor": "Best Paper Award in EWSN", "organization": "", "year": 2021}, {"honor": "Intel's 2020 Rising Star Faculty Award", "organization": "", "year": 2020}, {"honor": "Amazon Research Award", "organization": "", "year": 2020}, {"honor": "NSF CAREER Award", "organization": "National Science Foundation", "year": 2021}, {"honor": "MIT Technology Review 35 Innovators Under 35", "organization": "", "year": 2020}, {"honor": "Facebook Research Award", "organization": "", "year": 2019}, {"honor": "Amazon Research Award", "organization": "", "year": 2019}, {"honor": "IBM Research Award", "organization": "", "year": 2019}, {"honor": "Q4 AWS Machine Learning Research Awards", "organization": "", "year": 2019}, {"honor": "2022 Sloan Research Fellowship", "organization": "Alfred P. Sloan Foundation", "year": 2022}], "coursesTaught": [{"code": "CS 307", "title": "Model & Learning in Data Sci"}, {"code": "CS 442 (CS 498 LB1, CS 498 LB2)", "title": "Trustworthy Machine Learning"}, {"code": "CS 562", "title": "Adv Topics in Sec, Priv and ML"}, {"code": "CS 598 BL", "title": "Adversarial Machine Learning"}]}