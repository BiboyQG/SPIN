{"fullname": "Nan Jiang", "title": "Associate Professor", "contact": {"phone": "(217) 300-8275", "email": "nanjiang@illinois.edu"}, "office": "3322 Siebel Center for Comp Sci", "education": [{"degree": "PhD", "field": "Computer Science and Engineering", "institution": "University of Michigan", "year": 2017}, {"degree": "Bachelor of Engineering", "field": "Automation", "institution": "Tsinghua University", "year": 2011}], "biography": "Nan Jiang is an Associate Professor at the University of Illinois at Urbana-Champaign. His research focuses on building the theoretical foundation of reinforcement learning (RL), especially in the function-approximation setting. He has worked on various topics in RL, including off-policy evaluation, offline reinforcement learning, and reinforcement learning in low-rank MDPs. Jiang is also part of the new AI Institute for Future Edge Networks and Distributed Intelligence (AI-EDGE), which aims to design future generations of wireless edge networks that are highly efficient, reliable, robust, and secure.", "professionalHighlights": [{"position": "Associate Professor", "organization": "CS @ UIUC", "yearStart": 2024, "yearEnd": null}, {"position": "Assistant Professor", "organization": "CS @ UIUC", "yearStart": 2018, "yearEnd": 2024}, {"position": "Postdoc Researcher", "organization": "MSR NYC", "yearStart": 2017, "yearEnd": 2018}, {"position": "PhD", "organization": "CSE @ UMich", "yearStart": 2011, "yearEnd": 2017}], "researchStatement": "My research focuses on building the theoretical foundation of reinforcement learning (RL), especially in the function-approximation setting. I have worked on various topics in RL, including off-policy evaluation, offline reinforcement learning, and reinforcement learning in low-rank MDPs. My goal is to develop novel RL algorithms and theories that can be applied to real-world problems, such as optimizing computer network operations. I am excited to collaborate with researchers from other universities as part of the AI-EDGE institute to explore the synergies between networking and AI.", "researchInterests": [{"area": "Reinforcement Learning", "description": "I work on building the theoretical foundation of reinforcement learning (RL), especially in the function-approximation setting."}], "researchAreas": ["Artificial Intelligence"], "publications": [{"title": "Contextual Decision Processes with low Bellman rank are PAC-Learnable", "authors": ["Nan Jiang", "Akshay Krishnamurthy", "Alekh Agarwal", "John Langford", "Robert E. Schapire"], "conference": "Proceedings of the 34th International Conference on Machine Learning", "year": 2017}, {"title": "On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation", "authors": ["Yuheng Zhang", "Nan Jiang"], "conference": "NeurIPS-24", "year": 2024}, {"title": "Future-Dependent Value-Based Off-Policy Evaluation in POMDPs", "authors": ["Masatoshi Uehara", "Haruka Kiyohara", "Andrew Bennett", "Victor Chernozhukov", "Nan Jiang", "Nathan Kallus", "Chengchun Shi", "Wen Sun"], "conference": "NeurIPS-23", "year": 2023}, {"title": "Reinforcement Learning in Low-Rank MDPs with Density Features", "authors": ["Audrey Huang", "Jinglin Chen", "Nan Jiang"], "conference": "ICML-23", "year": 2023}, {"title": "Offline Reinforcement Learning with Realizability and Single-policy Concentrability", "authors": ["Wenhao Zhan", "Baihe Huang", "Audrey Huang", "Nan Jiang", "Jason D. Lee"], "conference": "COLT-22", "year": 2022}, {"title": "Adversarially Trained Actor Critic for Offline Reinforcement Learning", "authors": ["Ching-An Cheng", "Tengyang Xie", "Nan Jiang", "Alekh Agarwal"], "conference": "ICML-22", "year": 2022}, {"title": "The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation", "authors": ["Amortila", "Nan Jiang", "Szepesv\u00e1ri"], "conference": "ICML-23", "year": 2023}, {"title": "Adversarial Model for Offline Reinforcement Learning", "authors": ["Bhardwaj", "Xie", "Boots", "Nan Jiang", "Cheng"], "conference": "NeurIPS-23", "year": 2023}, {"title": "Offline reinforcement learning under value and density-ratio realizability: The power of gaps", "authors": ["Jinglin Chen", "Nan Jiang"], "conference": "UAI-22", "year": 2022}, {"title": "Beyond the Return: Off-policy Function Estimation under User-specified Error-measuring Distributions", "authors": ["Audrey Huang", "Nan Jiang"], "conference": "NeurIPS-22", "year": 2022}, {"title": "Tiered Reinforcement Learning: Pessimism in the Face of Uncertainty and Constant Regret", "authors": ["Jiawei Huang", "Li Zhao", "Tao Qin", "Wei Chen", "Nan Jiang", "Tie-Yan Liu"], "conference": "NeurIPS-22", "year": 2022}, {"title": "A Minimax Learning Approach to Off-Policy Evaluation in Confounded Partially Observable Markov Decision Processes", "authors": ["Chengchun Shi", "Masatoshi Uehara", "Jiawei Huang", "Nan Jiang"], "conference": "ICML-22", "year": 2022}, {"title": "THE ROLE OF COVERAGE IN ONLINE REINFORCEMENT LEARNING", "authors": ["Tengyang Xie", "Dylan Foster", "Yuxin Bai", "Nan Jiang", "Sachin Kakade"], "conference": "ICLR-23", "year": 2023}, {"title": "Towards Hyperparameter-free Policy Selection for Offline Reinforcement Learning", "authors": ["Siyuan Zhang", "Nan Jiang"], "conference": "NeurIPS-21", "year": 2021}, {"title": "Offline Learning in Markov Games with General Function Approximation", "authors": ["Yuheng Zhang", "Yuxin Bai", "Nan Jiang"], "conference": "ICML-23", "year": 2023}, {"title": "Offline Reinforcement Learning with Realizability and Single-policy Concentrability", "authors": ["Wenhao Zhan", "Baihe Huang", "Audrey Huang", "Nan Jiang", "Jason D. Lee"], "conference": "COLT-22", "year": 2022}], "teachingHonors": [{"honor": "Teachers Ranked as Excellent (Excellent)", "year": 2023}, {"honor": "Teachers Ranked as Excellent (Excellent)", "year": 2022}, {"honor": "Engineering Council Outstanding Advisor Award", "year": 2022}, {"honor": "Teachers Ranked as Excellent (Excellent)", "year": 2021}, {"honor": "Teachers Ranked as Excellent (Excellent)", "year": 2021}, {"honor": "Teachers Ranked as Excellent (Outstanding)", "year": 2020}, {"honor": "Teachers Ranked as Excellent (Excellent)", "year": 2019}, {"honor": "Teachers Ranked as Excellent (Excellent)", "year": 2018}], "researchHonors": [{"honor": "Google Research Scholar Award", "organization": "Google", "year": 2024}, {"honor": "Sloan Research Fellowship", "organization": "Sloan Foundation", "year": 2024}, {"honor": "ICML 2022 Outstanding Paper Runner Up", "organization": "ICML", "year": 2022}, {"honor": "NSF CAREER Award", "organization": "NSF", "year": 2022}, {"honor": "AAMAS 2015 Best Paper Award", "organization": "AAMAS", "year": 2015}], "coursesTaught": [{"code": "CS 443", "title": "Reinforcement Learning"}, {"code": "CS 542", "title": "Stat Reinforcement Learning"}, {"code": "CS 598 NJ", "title": "Stat Reinforcement lrng"}, {"code": "CS 598 NJ", "title": "Statistical Reinforcement"}]}